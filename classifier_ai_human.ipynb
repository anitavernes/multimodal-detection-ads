{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20MEbNcrrOc3",
    "outputId": "e7587bf9-c343-4f0c-9344-d23339d82a29"
   },
   "outputs": [],
   "source": [
    "pip install pytesseract pillow transformers torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aG8KV8-GrYbR"
   },
   "source": [
    "<h1>1. Setup and Library Imports</h1>\n",
    "This section handles mounting Google Drive (if running in Colab) and importing all necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57PlP0curUsI",
    "outputId": "ae001aa3-8b99-42c8-fc04-970225e83457"
   },
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mySUEJCrwdh"
   },
   "source": [
    "<h1>2. Model Loading</h1>\n",
    "Here, the pre-trained image and text classification models are loaded. The model file (ai_vs_human_ads_classifier_finetuned.pth) is accessible in Google Drive path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417,
     "referenced_widgets": [
      "2734372d189b4205aa4120dd3c400b64",
      "b9e2b2abd33440228732da22bb1d059f",
      "8e767a8846a74292a88353643295d2ad",
      "05bcc785df5b438e8c1df29f7f3dc171",
      "f92795298175420482a85e32a03d52cc",
      "a342fdc85d7f4ea4b6ce673f26c3e343",
      "611e5111ea9d4e7390608a732884877c",
      "48b1db1bc1024559949a2ebb3a2fd0a1",
      "7d520845d1c24381923efbda5fb57296",
      "96bb9dd03180493082053ecc3d763721",
      "ee362e7cae99487bbb2ad1b28f8c3e02",
      "9e032e686a89468e902dca122542a236",
      "ae04f81c368f45b18f8ef3426eb13522",
      "7ce256fc803a43a7bed39648bf056505",
      "70adcea29ec641ebb6f7eca87ea95a4a",
      "f9fb759403cc446c94889d183737ad17",
      "5989e389ff144be69977aca03ece4de9",
      "756c6f4cef0b4671ad4651770cb3cc44",
      "7d39766b1feb47839cec7c5fe43e0888",
      "6314a7dd4d024a18b2a075cf3cffa097",
      "c893506512f7409ba580cfc4cec0d439",
      "a631a2e8e2d641d387875a52cbc29812",
      "b990529b7d77447783e68acb49e30449",
      "6ca6ab3298eb439ea1067a0713486989",
      "ae9cc571bbf3429da6a0206053317cd5",
      "34f89086424e46afbeaacefccce4b65c",
      "3f0ddffa94b6416382efc1b1cb3c400d",
      "49207fd578a84ef0b219f5c4476f1204",
      "c46e4e94255c42119dbaae17c4018c79",
      "77220a118f6d4dada06cff3325a7af10",
      "fcd716fbe02149adb384b9551cfea765",
      "4ce8afe1880a4fa784e1567cd659172a",
      "b1435de773364b8dafad002810e312e4",
      "9aab42857a314e7293a57f77950907d3",
      "89a1a3614336455dac9ec774dd3ce919",
      "59c78fa905974959bf81f36ef5dc9fc0",
      "b02a06aa411d4514bf217c2258032012",
      "8c93dcde0333454bad2c81bead38f01f",
      "1efdf36a921645b69fb8299a535320dd",
      "b7563e32be6541e4b4b23c5ba27e64bb",
      "a62125198c384e6b9f2ebe5c85312d21",
      "b059d3c5af7041a49fa79078f1bf71d4",
      "ec9c6e40a47f42d68bebbeaf0203d0d3",
      "295f70eae93a40339811c223c146a864",
      "acfe26227e0b490aa7381a0302eda116",
      "4859d0439b6a40ecbd246a4a4c7bd379",
      "ba892d170568427081cb93ebce8950d4",
      "0bd0954b250749a9910b98894595b836",
      "7976bc9e049b40588573712c83ced1db",
      "75d4d935883b43eabb1cde378e7601df",
      "c0a5a91ff0f241de8e9d79cee5bb5f46",
      "a4476ba2c33f44d08f2849fef35df36f",
      "3710d4df4f644181a89661c0fe8171d1",
      "955b75f2ea2040b2bd48bc6cc7918f29",
      "b9f1aa12afc445578e0f241328ad9f2f",
      "949bb98f152a4997a0c444a886af5186",
      "10ae1633bc6a469bba53a90704b52386",
      "de39e7dbb5b548ae85e9340bcc389b2d",
      "91ee6cbdccfa43c28bea31f9b7c82c15",
      "c893fe0e204f4108a2b1255d8f88ca84",
      "229caecf676749db9fd926cda3d30c44",
      "16f01ac4108a43e4ad54bcde44cf9078",
      "b99d61a55b0e4cffae5109751bc31a9b",
      "fe861dadd973488ca3ac4e6a54829794",
      "066b471a5d514344b71597b851814ef0",
      "04a5650d743642be99ea6d1697f4a884",
      "4fbfd60e14014b1a98945acbe19ef132",
      "94182d640d494f77be5b64e1cd53e185",
      "b51cd00ef8ec45e5bf1795f797c71f50",
      "723a096a9d654bd78216562d7f5fac10",
      "84cc307778c149e7a5932a10dbc7889b",
      "73d5d72bde144889b5d87297a3efa7d3",
      "43189c0cbc90472c88c8568df5c5b381",
      "09bb4b147111407ca6178c3653b08b97",
      "2f1eb21c266c4fa98d16689d7bd4dbd4",
      "2b25656709e64e9abf73f22fad0805a4",
      "312310bfeacc410dbe6f6070262cd7ad"
     ]
    },
    "id": "DmenjUnprrJY",
    "outputId": "ac10ccc4-9719-45b9-97cf-b7121178ee29"
   },
   "outputs": [],
   "source": [
    "# Load image classifier\n",
    "image_model = timm.create_model('resnet18', pretrained=True)\n",
    "image_model.fc = torch.nn.Linear(image_model.fc.in_features, 2)\n",
    "image_model.load_state_dict(torch.load('drive/MyDrive/ai_vs_human_ads_classifier_finetuned.pth'))\n",
    "image_model.eval()\n",
    "\n",
    "# Load text classifier\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base-openai-detector\")\n",
    "text_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base-openai-detector\")\n",
    "text_model.eval()\n",
    "\n",
    "# Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Global list to store results for batch analysis\n",
    "analysis_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvMzUP-5ryTU"
   },
   "source": [
    "<h1>3. Core Prediction Functions</h1>\n",
    "These functions encapsulate the logic for image prediction, text classification, and text extraction. Each function is designed to be self-contained and return detailed results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2L38reQuQrT"
   },
   "source": [
    "<h2>Image Prediction Function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGH4WSCxrpuE"
   },
   "outputs": [],
   "source": [
    "def predict_image(path):\n",
    "    \"\"\"Enhanced image prediction with detailed probabilities\"\"\"\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    img_t = transform(image).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = image_model(img_t)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        pred = torch.argmax(probs, 1).item()\n",
    "\n",
    "        # Get probabilities for both classes\n",
    "        real_prob = probs[0][0].item()\n",
    "        ai_prob = probs[0][1].item()\n",
    "\n",
    "    return {\n",
    "        'prediction': ['Real', 'AI-Generated'][pred],\n",
    "        'confidence': probs[0][pred].item(),\n",
    "        'real_probability': real_prob,\n",
    "        'ai_probability': ai_prob,\n",
    "        'raw_output': outputs[0].tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6cI8O8RuWa5"
   },
   "source": [
    "<h2>Text Classification Function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuiR17lmuak7"
   },
   "outputs": [],
   "source": [
    "def classify_text(text, threshold=0.8):\n",
    "    \"\"\"Enhanced text classification with detailed metrics\"\"\"\n",
    "    if not text.strip():\n",
    "        return {\n",
    "            'prediction': 'No text detected',\n",
    "            'confidence': 0.0,\n",
    "            'human_probability': 0.0,\n",
    "            'ai_probability': 0.0,\n",
    "            'text_length': 0,\n",
    "            'word_count': 0\n",
    "        }\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = text_model(**inputs)\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0][pred].item()\n",
    "\n",
    "        # Get probabilities for both classes\n",
    "        human_prob = probs[0][0].item()\n",
    "        ai_prob = probs[0][1].item()\n",
    "\n",
    "        label = [\"Human-written\", \"AI-generated\"][pred]\n",
    "        if confidence < threshold:\n",
    "            label = \"Uncertain\"\n",
    "\n",
    "    return {\n",
    "        'prediction': label,\n",
    "        'confidence': confidence,\n",
    "        'human_probability': human_prob,\n",
    "        'ai_probability': ai_prob,\n",
    "        'text_length': len(text),\n",
    "        'word_count': len(text.split()),\n",
    "        'raw_output': outputs.logits[0].tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYKw6PVBueLk"
   },
   "source": [
    "<h2>Text Extraction Function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GwKUOkrugOp"
   },
   "outputs": [],
   "source": [
    "def extract_text(image_path):\n",
    "    \"\"\"Enhanced text extraction with preprocessing metrics\"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        original_size = image.size\n",
    "\n",
    "        # Resize\n",
    "        image = image.resize((image.width * 2, image.height * 2), Image.LANCZOS)\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray = image.convert(\"L\")\n",
    "\n",
    "        # Apply OpenCV processing\n",
    "        img_cv = np.array(gray)\n",
    "        _, thresh = cv2.threshold(img_cv, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "        # Convert back to PIL\n",
    "        processed = Image.fromarray(thresh)\n",
    "\n",
    "        # OCR with config\n",
    "        config = r'--oem 3 --psm 6'\n",
    "        text = pytesseract.image_to_string(processed, config=config)\n",
    "\n",
    "        return {\n",
    "            'text': text.strip(),\n",
    "            'original_image_size': original_size,\n",
    "            'processed_image_size': processed.size,\n",
    "            'character_count': len(text.strip()),\n",
    "            'word_count': len(text.strip().split()) if text.strip() else 0\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'text': '',\n",
    "            'original_image_size': (0, 0),\n",
    "            'processed_image_size': (0, 0),\n",
    "            'character_count': 0,\n",
    "            'word_count': 0,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ov5qE9Qxuj5h"
   },
   "source": [
    "<h1>4. Composite Analysis and Reporting Functions</h1>\n",
    "These functions tie together the image and text analyses, calculate a composite score, make a final decision, and handle result storage and reporting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKW83bgavJ51"
   },
   "source": [
    "<h2>Composite Score Calculation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymGQgSIFul8x"
   },
   "outputs": [],
   "source": [
    "def calculate_composite_score(img_results, txt_results):\n",
    "    \"\"\"Calculate a composite AI detection score\"\"\"\n",
    "    # Weights for different components\n",
    "    img_weight = 0.6\n",
    "    txt_weight = 0.4\n",
    "\n",
    "    # Get AI probabilities\n",
    "    img_ai_prob = img_results['ai_probability']\n",
    "    txt_ai_prob = txt_results['ai_probability'] if txt_results['prediction'] != 'No text detected' else 0.5\n",
    "\n",
    "    # Calculate weighted score\n",
    "    composite_score = (img_ai_prob * img_weight) + (txt_ai_prob * txt_weight)\n",
    "\n",
    "    return composite_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_T1UUPEuqTP"
   },
   "source": [
    "<h2>Full Analysis Function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmTZ6MsLut3s"
   },
   "outputs": [],
   "source": [
    "def full_analysis(image_path, ground_truth=None, save_results=True):\n",
    "    # Image analysis\n",
    "    img_results = predict_image(image_path)\n",
    "\n",
    "    # Text extraction\n",
    "    text_data = extract_text(image_path)\n",
    "\n",
    "    # Text analysis\n",
    "    txt_results = classify_text(text_data['text'])\n",
    "\n",
    "    # Composite analysis\n",
    "    composite_score = calculate_composite_score(img_results, txt_results)\n",
    "\n",
    "    # Final decision with numerical thresholds\n",
    "    if composite_score > 0.7:\n",
    "        final_decision = \"Highly Likely AI-Generated\"\n",
    "        confidence_level = \"High\"\n",
    "    elif composite_score > 0.5:\n",
    "        final_decision = \"Likely AI-Generated\"\n",
    "        confidence_level = \"Medium\"\n",
    "    elif composite_score > 0.3:\n",
    "        final_decision = \"Uncertain\"\n",
    "        confidence_level = \"Low\"\n",
    "    else:\n",
    "        final_decision = \"Likely Human-Generated\"\n",
    "        confidence_level = \"Medium-High\"\n",
    "\n",
    "\n",
    "    # Final label for comparison\n",
    "    if 'AI' in final_decision:\n",
    "        final_label = 'AI-Generated'\n",
    "    elif 'Human' in final_decision:\n",
    "        final_label = 'Human-Generated'\n",
    "    else:\n",
    "        final_label = 'Unknown'\n",
    "\n",
    "    # Store results for batch analysis\n",
    "    if save_results:\n",
    "        result_entry = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'image_path': image_path,\n",
    "            'img_prediction': img_results['prediction'],\n",
    "            'img_confidence': img_results['confidence'],\n",
    "            'img_real_prob': img_results['real_probability'],\n",
    "            'img_ai_prob': img_results['ai_probability'],\n",
    "            'text_extracted': text_data['character_count'] > 0,\n",
    "            'text_char_count': text_data['character_count'],\n",
    "            'text_word_count': text_data['word_count'],\n",
    "            'txt_prediction': txt_results['prediction'],\n",
    "            'txt_confidence': txt_results['confidence'],\n",
    "            'txt_human_prob': txt_results['human_probability'],\n",
    "            'txt_ai_prob': txt_results['ai_probability'],\n",
    "            'composite_score': composite_score,\n",
    "            'final_decision': final_decision,\n",
    "            'confidence_level': confidence_level,\n",
    "            'ground_truth': ground_truth,\n",
    "            'img_correct': img_results['prediction'] == ground_truth,\n",
    "            'txt_correct': (txt_results['prediction'] == ground_truth) if text_data['character_count'] > 0 else None,\n",
    "            'final_correct': final_label == ground_truth\n",
    "        }\n",
    "        analysis_results.append(result_entry)\n",
    "\n",
    "    return result_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlIZ_IxjwjMc"
   },
   "source": [
    "<h2>Batch Analysis Function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZ9vaf6IsdxI"
   },
   "outputs": [],
   "source": [
    "def batch_analysis(image_entries):\n",
    "    \"\"\"Analyze multiple images and return comprehensive statistics\"\"\"\n",
    "    print(f\"\\n🔄 BATCH ANALYSIS: Processing {len(image_entries)} images...\")\n",
    "\n",
    "    batch_results = []\n",
    "    for entry in image_entries:\n",
    "        try:\n",
    "            # Extract path and ground_truth from dict, provide default if missing\n",
    "            path = entry['path'] if isinstance(entry, dict) else entry\n",
    "            ground_truth = entry.get('ground_truth', 'Unknown') if isinstance(entry, dict) else 'Unknown'\n",
    "\n",
    "            result = full_analysis(path, ground_truth=ground_truth)\n",
    "            batch_results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {entry}: {e}\")\n",
    "\n",
    "    return batch_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRwNbzPewptX"
   },
   "source": [
    "<h2>Statistics Report Generation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2539B1TwoBJ"
   },
   "outputs": [],
   "source": [
    "def generate_statistics_report():\n",
    "    \"\"\"Generate comprehensive statistics from all analysis results\"\"\"\n",
    "    if not analysis_results:\n",
    "        print(\"No analysis results available. Run some analyses first.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(analysis_results)\n",
    "\n",
    "    print(f\"\\n📊 STATISTICS REPORT\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total Images Analyzed: {len(df)}\")\n",
    "    print(f\"Date Range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "\n",
    "    # Image Classification Stats\n",
    "    print(f\"\\n📷 IMAGE CLASSIFICATION:\")\n",
    "    img_pred_counts = df['img_prediction'].value_counts()\n",
    "    for pred, count in img_pred_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   {pred}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "    print(f\"   Average Image Confidence: {df['img_confidence'].mean():.4f}\")\n",
    "    print(f\"   Average AI Probability: {df['img_ai_prob'].mean():.4f}\")\n",
    "\n",
    "    # Text Analysis Stats\n",
    "    print(f\"\\n📝 TEXT ANALYSIS:\")\n",
    "    text_detected = df['text_extracted'].sum()\n",
    "    print(f\"   Images with Text: {text_detected} ({(text_detected/len(df)*100):.1f}%)\")\n",
    "    print(f\"   Average Characters per Image: {df['text_char_count'].mean():.1f}\")\n",
    "    print(f\"   Average Words per Image: {df['text_word_count'].mean():.1f}\")\n",
    "\n",
    "    # Text classification for images with text\n",
    "    text_df = df[df['text_extracted']]\n",
    "    if len(text_df) > 0:\n",
    "        txt_pred_counts = text_df['txt_prediction'].value_counts()\n",
    "        for pred, count in txt_pred_counts.items():\n",
    "            percentage = (count / len(text_df)) * 100\n",
    "            print(f\"   Text {pred}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "    # Composite Scores\n",
    "    print(f\"\\n🎯 COMPOSITE ANALYSIS:\")\n",
    "    print(f\"   Average Composite Score: {df['composite_score'].mean():.4f}\")\n",
    "    print(f\"   Median Composite Score: {df['composite_score'].median():.4f}\")\n",
    "    print(f\"   Score Standard Deviation: {df['composite_score'].std():.4f}\")\n",
    "\n",
    "    # Final Decisions\n",
    "    print(f\"\\n🏁 FINAL DECISIONS:\")\n",
    "    final_counts = df['final_decision'].value_counts()\n",
    "    for decision, count in final_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   {decision}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VurXRC9wzeG"
   },
   "source": [
    "<h2>Plotting Analysis Results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eg7vWzijwxep"
   },
   "outputs": [],
   "source": [
    "def plot_analysis_results():\n",
    "    \"\"\"Create visualizations of the analysis results\"\"\"\n",
    "    if not analysis_results:\n",
    "        print(\"No analysis results available for plotting.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(analysis_results)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Plot 1: Composite Score Distribution\n",
    "    axes[0, 0].hist(df['composite_score'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Distribution of Composite AI Scores')\n",
    "    axes[0, 0].set_xlabel('Composite Score')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].axvline(df['composite_score'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"composite_score\"].mean():.3f}')\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    # Plot 2: Image vs Text AI Probabilities\n",
    "    axes[0, 1].scatter(df['img_ai_prob'], df['txt_ai_prob'], alpha=0.6)\n",
    "    axes[0, 1].set_title('Image AI Prob vs Text AI Prob')\n",
    "    axes[0, 1].set_xlabel('Image AI Probability')\n",
    "    axes[0, 1].set_ylabel('Text AI Probability')\n",
    "    axes[0, 1].plot([0, 1], [0, 1], 'r--', alpha=0.5)\n",
    "\n",
    "    # Plot 3: Final Decision Counts\n",
    "    decision_counts = df['final_decision'].value_counts()\n",
    "    axes[1, 0].pie(decision_counts.values, labels=decision_counts.index, autopct='%1.1f%%')\n",
    "    axes[1, 0].set_title('Final Decision Distribution')\n",
    "\n",
    "    # Plot 4: Confidence vs Composite Score\n",
    "    confidence_map = {'Low': 1, 'Medium': 2, 'Medium-High': 3, 'High': 4}\n",
    "    df['confidence_numeric'] = df['confidence_level'].map(confidence_map)\n",
    "    axes[1, 1].scatter(df['composite_score'], df['confidence_numeric'], alpha=0.6)\n",
    "    axes[1, 1].set_title('Composite Score vs Confidence Level')\n",
    "    axes[1, 1].set_xlabel('Composite Score')\n",
    "    axes[1, 1].set_ylabel('Confidence Level')\n",
    "    axes[1, 1].set_yticks([1, 2, 3, 4])\n",
    "    axes[1, 1].set_yticklabels(['Low', 'Medium', 'Medium-High', 'High'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yz9yGjJtw4-M"
   },
   "source": [
    "<h2>Binary Classification Metrics</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ef_5fqokw3ak"
   },
   "outputs": [],
   "source": [
    "def plot_binary_classification_metrics(df, true_col, pred_col, title_suffix, ground_truth_labels, pred_mapping=None):\n",
    "    \"\"\"\n",
    "    Plots confusion matrix and prints classification report for binary classification.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the analysis results.\n",
    "        true_col (str): The name of the column containing the ground truth labels.\n",
    "        pred_col (str): The name of the column containing the predicted labels.\n",
    "        title_suffix (str): A suffix to add to the plot title (e.g., \"Image Analysis\").\n",
    "        ground_truth_labels (list): A list of the two expected ground truth labels (e.g., [\"Human-Generated\", \"AI-Generated\"]).\n",
    "        pred_mapping (dict, optional): A dictionary to map predicted labels to ground truth labels.\n",
    "                                       Defaults to None, in which case direct comparison is used.\n",
    "    \"\"\"\n",
    "    # Filter for valid ground truth entries\n",
    "    valid_df = df[df[true_col].isin(ground_truth_labels)].copy()\n",
    "\n",
    "    if valid_df.empty:\n",
    "        print(f\"\\n❗ Not enough valid data for {title_suffix} confusion matrix and report.\")\n",
    "        return\n",
    "\n",
    "    y_true = valid_df[true_col]\n",
    "    y_pred_raw = valid_df[pred_col]\n",
    "\n",
    "    # Apply mapping if provided\n",
    "    if pred_mapping:\n",
    "        y_pred = y_pred_raw.map(pred_mapping)\n",
    "        # Filter out predictions that couldn't be mapped to the ground truth labels (e.g., 'Uncertain')\n",
    "        combined_df = pd.DataFrame({'true': y_true, 'pred': y_pred}).dropna()\n",
    "        y_true = combined_df['true']\n",
    "        y_pred = combined_df['pred']\n",
    "    else:\n",
    "        # Filter out any predictions not directly matching ground truth labels\n",
    "        combined_df = pd.DataFrame({'true': y_true, 'pred': y_pred_raw})\n",
    "        combined_df = combined_df[combined_df['pred'].isin(ground_truth_labels)].dropna()\n",
    "        y_true = combined_df['true']\n",
    "        y_pred = combined_df['pred']\n",
    "\n",
    "    if y_true.empty or y_pred.empty:\n",
    "        print(f\"\\n❗ After filtering, not enough binary data for {title_suffix} confusion matrix and report.\")\n",
    "        return\n",
    "\n",
    "    # Ensure consistent order for labels in confusion matrix\n",
    "    display_labels = ground_truth_labels\n",
    "\n",
    "    print(f\"\\n--- {title_suffix} Performance ---\")\n",
    "    print(f\"\\n📉 Confusion Matrix for {title_suffix}:\")\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=display_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format='d')\n",
    "    plt.title(f\"Confusion Matrix: Ground Truth vs {title_suffix} Prediction\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n📋 Classification Report for {title_suffix}:\")\n",
    "    print(classification_report(y_true, y_pred, labels=display_labels, target_names=display_labels, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afDJG5sBw_91"
   },
   "source": [
    "<h2>Export Results to CSV</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_W6zAFJw89r"
   },
   "outputs": [],
   "source": [
    "def export_results_to_csv(filename='ai_detection_results.csv'):\n",
    "    \"\"\"Export analysis results to CSV file\"\"\"\n",
    "    if not analysis_results:\n",
    "        print(\"No results to export.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(analysis_results)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Results exported to {filename}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XZa8TeexEwH"
   },
   "source": [
    "<h2>5. Running the Analysis</h2>\n",
    "This is the main execution block where you define your image paths, run the batch analysis, and then call the reporting and plotting functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNxBiMRMCi-o"
   },
   "source": [
    "<h2>5.1. Running analysis on full dataset (both human + ai generated images)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "inWOwB4yxCou",
    "outputId": "a9844865-dbcd-428c-b690-0b74d1ffa954"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Paths to folders (adjust if your folder names are different)\n",
    "real_folder = \"drive/MyDrive/real_img\"\n",
    "ai_folder = \"drive/MyDrive/ai_img\"\n",
    "\n",
    "# Build labeled image list for batch processing\n",
    "image_entries = []\n",
    "\n",
    "# Add real images with ground truth label\n",
    "for filename in os.listdir(real_folder):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_entries.append({\n",
    "            'path': os.path.join(real_folder, filename),\n",
    "            'ground_truth': 'Human-Generated'\n",
    "        })\n",
    "\n",
    "# Add AI-generated images\n",
    "for filename in os.listdir(ai_folder):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_entries.append({\n",
    "            'path': os.path.join(ai_folder, filename),\n",
    "            'ground_truth': 'AI-Generated'\n",
    "        })\n",
    "\n",
    "# Now run batch analysis on all images\n",
    "batch_results = batch_analysis(image_entries)\n",
    "\n",
    "# Generate overall statistics report and plots\n",
    "df = generate_statistics_report()\n",
    "plot_analysis_results()\n",
    "\n",
    "\n",
    "# Common ground truth labels for both analyses\n",
    "GT_LABELS = ['Human-Generated', 'AI-Generated']\n",
    "\n",
    "# 1. Performance for Image Analysis\n",
    "# Map 'Real' prediction to 'Human-Generated' ground truth\n",
    "image_pred_mapping = {'Real': 'Human-Generated', 'AI-Generated': 'AI-Generated'}\n",
    "plot_binary_classification_metrics(\n",
    "    df=df,\n",
    "    true_col='ground_truth',\n",
    "    pred_col='img_prediction',\n",
    "    title_suffix='Image Analysis',\n",
    "    ground_truth_labels=GT_LABELS,\n",
    "    pred_mapping=image_pred_mapping\n",
    ")\n",
    "\n",
    "# 2. Performance for Text Analysis\n",
    "# Filter for images where text was actually extracted for meaningful text analysis\n",
    "text_analysis_df = df[df['text_extracted']].copy()\n",
    "# Map 'Human-written' to 'Human-Generated' and 'AI-generated' to 'AI-Generated'\n",
    "# 'Uncertain' will be filtered out by the plot_binary_classification_metrics function\n",
    "text_pred_mapping = {'Human-written': 'Human-Generated', 'AI-generated': 'AI-Generated'}\n",
    "plot_binary_classification_metrics(\n",
    "    df=text_analysis_df,\n",
    "    true_col='ground_truth',\n",
    "    pred_col='txt_prediction',\n",
    "    title_suffix='Text Analysis',\n",
    "    ground_truth_labels=GT_LABELS,\n",
    "    pred_mapping=text_pred_mapping\n",
    ")\n",
    "\n",
    "# 3. Performance for Final Composite Decision (original plot_confusion_and_classification logic)\n",
    "# This uses the final_decision which is already normalized to 'AI-Generated' or 'Human-Generated'\n",
    "# by the internal logic of plot_confusion_and_classification, but we can use our new function for consistency.\n",
    "# For this, we'll need to create a temporary column that maps 'Likely Human-Generated', etc. to the binary labels\n",
    "df['final_binary_decision'] = df['final_decision'].apply(\n",
    "    lambda x: 'AI-Generated' if 'AI' in x else ('Human-Generated' if 'Human' in x else np.nan)\n",
    ")\n",
    "plot_binary_classification_metrics(\n",
    "    df=df,\n",
    "    true_col='ground_truth',\n",
    "    pred_col='final_binary_decision',\n",
    "    title_suffix='Final Composite Decision Analysis',\n",
    "    ground_truth_labels=GT_LABELS\n",
    ")\n",
    "\n",
    "\n",
    "# Export results to CSV\n",
    "export_results_to_csv(\"combined_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZHbbi5bDCub"
   },
   "source": [
    "<h2>5.2. Running analysis on Human-generated dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KfjzhutHCfrM",
    "outputId": "cdca5c5e-6ada-43eb-f2af-e76fdbd162d9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Paths to folders\n",
    "real_folder = \"drive/MyDrive/real_img\"\n",
    "analysis_results = []\n",
    "# Build labeled image list for batch processing\n",
    "image_entries = []\n",
    "\n",
    "# Add real images with ground truth label\n",
    "for filename in os.listdir(real_folder):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_entries.append({\n",
    "            'path': os.path.join(real_folder, filename),\n",
    "            'ground_truth': 'Human-Generated'\n",
    "        })\n",
    "\n",
    "# Now run batch analysis on all images\n",
    "batch_results = batch_analysis(image_entries)\n",
    "\n",
    "# Generate overall statistics report and plots\n",
    "df = generate_statistics_report()\n",
    "plot_analysis_results()\n",
    "\n",
    "\n",
    "# Common ground truth labels for both analyses\n",
    "GT_LABELS = ['Human-Generated', 'AI-Generated']\n",
    "\n",
    "# 1. Performance for Image Analysis\n",
    "# Map 'Real' prediction to 'Human-Generated' ground truth\n",
    "image_pred_mapping = {'Real': 'Human-Generated', 'AI-Generated': 'AI-Generated'}\n",
    "plot_binary_classification_metrics(\n",
    "    df=df,\n",
    "    true_col='ground_truth',\n",
    "    pred_col='img_prediction',\n",
    "    title_suffix='Image Analysis',\n",
    "    ground_truth_labels=GT_LABELS,\n",
    "    pred_mapping=image_pred_mapping\n",
    ")\n",
    "\n",
    "# 2. Performance for Text Analysis\n",
    "# Filter for images where text was actually extracted for meaningful text analysis\n",
    "text_analysis_df = df[df['text_extracted']].copy()\n",
    "# Map 'Human-written' to 'Human-Generated' and 'AI-generated' to 'AI-Generated'\n",
    "# 'Uncertain' will be filtered out by the plot_binary_classification_metrics function\n",
    "text_pred_mapping = {'Human-written': 'Human-Generated', 'AI-generated': 'AI-Generated'}\n",
    "plot_binary_classification_metrics(\n",
    "    df=text_analysis_df,\n",
    "    true_col='ground_truth',\n",
    "    pred_col='txt_prediction',\n",
    "    title_suffix='Text Analysis',\n",
    "    ground_truth_labels=GT_LABELS,\n",
    "    pred_mapping=text_pred_mapping\n",
    ")\n",
    "\n",
    "# 3. Performance for Final Composite Decision (original plot_confusion_and_classification logic)\n",
    "# This uses the final_decision which is already normalized to 'AI-Generated' or 'Human-Generated'\n",
    "# by the internal logic of plot_confusion_and_classification, but we can use our new function for consistency.\n",
    "# For this, we'll need to create a temporary column that maps 'Likely Human-Generated', etc. to the binary labels\n",
    "df['final_binary_decision'] = df['final_decision'].apply(\n",
    "    lambda x: 'AI-Generated' if 'AI' in x else ('Human-Generated' if 'Human' in x else np.nan)\n",
    ")\n",
    "plot_binary_classification_metrics(\n",
    "    df=df,\n",
    "    true_col='ground_truth',\n",
    "    pred_col='final_binary_decision',\n",
    "    title_suffix='Final Composite Decision Analysis',\n",
    "    ground_truth_labels=GT_LABELS\n",
    ")\n",
    "\n",
    "\n",
    "# Export results to CSV\n",
    "export_results_to_csv(\"human_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0psm097DSj-"
   },
   "source": [
    "<h2>5.3. Running analysis on AI-generated dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rYVLIk6N0MKt",
    "outputId": "9c62306d-3a10-4d56-9bde-f408a4e5db10"
   },
   "outputs": [],
   "source": [
    "# Paths to folders\n",
    "ai_folder = \"drive/MyDrive/ai_img\"\n",
    "\n",
    "# Build labeled image list for batch processing\n",
    "image_entries = []\n",
    "analysis_results = []\n",
    "\n",
    "# Add AI-generated images\n",
    "for filename in os.listdir(ai_folder):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_entries.append({\n",
    "            'path': os.path.join(ai_folder, filename),\n",
    "            'ground_truth': 'AI-Generated'\n",
    "        })\n",
    "\n",
    "# Now run batch analysis on all images\n",
    "batch_results = batch_analysis(image_entries)\n",
    "\n",
    "# Generate overall statistics report and plots\n",
    "df = generate_statistics_report()\n",
    "plot_analysis_results()\n",
    "\n",
    "\n",
    "# Common ground truth labels for both analyses\n",
    "GT_LABELS = ['Human-Generated', 'AI-Generated']\n",
    "\n",
    "# 1. Performance for Image Analysis\n",
    "# Map 'Real' prediction to 'Human-Generated' ground truth\n",
    "image_pred_mapping = {'Real': 'Human-Generated', 'AI-Generated': 'AI-Generated'}\n",
    "plot_binary_classification_metrics(\n",
    "    df=df,\n",
    "    true_col='ground_truth',\n",
    "    pred_col='img_prediction',\n",
    "    title_suffix='Image Analysis',\n",
    "    ground_truth_labels=GT_LABELS,\n",
    "    pred_mapping=image_pred_mapping\n",
    ")\n",
    "\n",
    "# 2. Performance for Text Analysis\n",
    "# Filter for images where text was actually extracted for meaningful text analysis\n",
    "text_analysis_df = df[df['text_extracted']].copy()\n",
    "# Map 'Human-written' to 'Human-Generated' and 'AI-generated' to 'AI-Generated'\n",
    "# 'Uncertain' will be filtered out by the plot_binary_classification_metrics function\n",
    "text_pred_mapping = {'Human-written': 'Human-Generated', 'AI-generated': 'AI-Generated'}\n",
    "plot_binary_classification_metrics(\n",
    "    df=text_analysis_df,\n",
    "    true_col='ground_truth',\n",
    "    pred_col='txt_prediction',\n",
    "    title_suffix='Text Analysis',\n",
    "    ground_truth_labels=GT_LABELS,\n",
    "    pred_mapping=text_pred_mapping\n",
    ")\n",
    "\n",
    "# 3. Performance for Final Composite Decision (original plot_confusion_and_classification logic)\n",
    "# This uses the final_decision which is already normalized to 'AI-Generated' or 'Human-Generated'\n",
    "# by the internal logic of plot_confusion_and_classification, but we can use our new function for consistency.\n",
    "# For this, we'll need to create a temporary column that maps 'Likely Human-Generated', etc. to the binary labels\n",
    "df['final_binary_decision'] = df['final_decision'].apply(\n",
    "    lambda x: 'AI-Generated' if 'AI' in x else ('Human-Generated' if 'Human' in x else np.nan)\n",
    ")\n",
    "plot_binary_classification_metrics(\n",
    "    df=df,\n",
    "    true_col='ground_truth',\n",
    "    pred_col='final_binary_decision',\n",
    "    title_suffix='Final Composite Decision Analysis',\n",
    "    ground_truth_labels=GT_LABELS\n",
    ")\n",
    "\n",
    "\n",
    "# Export results to CSV\n",
    "export_results_to_csv(\"ai_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9JP0hhdul9p"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
